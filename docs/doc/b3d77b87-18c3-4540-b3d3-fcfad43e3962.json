{
    "summary": "MultiModalMambaBlock is a PyTorch module for multimodal data fusion. It utilizes an encoder-decoder architecture, supports various fusion methods and defines the Mamba model class with ViT encoder. The code performs different operations based on the selected fusion method.",
    "details": [
        {
            "comment": "The MultiModalMambaBlock is a PyTorch module that combines text and image embeddings using a multimodal fusion approach. It takes arguments such as dimension of embeddings, depth, dropout rate, number of attention heads, and other parameters for the Mamba block and encoder. The fusion method can be set to \"mlp\", \"concat\", or \"add\".",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/block.py\":0-22",
            "content": "import torch\nfrom torch import nn, Tensor\nfrom zeta.nn import VisualExpert, MLP\nfrom zeta.nn.modules.simple_mamba import MambaBlock\nfrom zeta.structs import ViTransformerWrapper, Encoder\nclass MultiModalMambaBlock(nn.Module):\n    \"\"\"\n    MultiModalMambaBlock is a PyTorch module that combines text and image embeddings using a multimodal fusion approach.\n    Args:\n        dim (int): The dimension of the embeddings.\n        depth (int): The depth of the Mamba block.\n        dropout (float): The dropout rate.\n        heads (int): The number of attention heads.\n        d_state (int): The dimension of the state in the Mamba block.\n        image_size (int): The size of the input image.\n        patch_size (int): The size of the image patches.\n        encoder_dim (int): The dimension of the encoder embeddings.\n        encoder_depth (int): The depth of the encoder.\n        encoder_heads (int): The number of attention heads in the encoder.\n        fusion_method (str): The multimodal fusion method to use. Can be one of [\"mlp\", \"concat\", \"add\"]."
        },
        {
            "comment": "This code defines a class called MultiModalMambaBlock, which is an encoder-decoder based architecture for multimodal data. It takes various parameters including dimensions, depth, dropout rate, number of heads, state dimension, image size, and patch size. It also accepts optional fusion method and expansion rate parameters.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/block.py\":24-69",
            "content": "    Examples:\n    x = torch.randn(1, 16, 64)\n    y = torch.randn(1, 3, 64, 64)\n    model = MultiModalMambaBlock(\n        dim = 64,\n        depth = 5,\n        dropout = 0.1,\n        heads = 4,\n        d_state = 16,\n        image_size = 64,\n        patch_size = 16,\n        encoder_dim = 64,\n        encoder_depth = 5,\n        encoder_heads = 4\n    )\n    out = model(x, y)\n    print(out.shape)\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        dropout: float,\n        heads: int,\n        d_state: int,\n        image_size: int,\n        patch_size: int,\n        encoder_dim: int,\n        encoder_depth: int,\n        encoder_heads: int,\n        fusion_method: str = \"mlp\",\n        expansion_rate: int = 2,\n        *args,\n        **kwargs,\n    ):\n        super(MultiModalMambaBlock, self).__init__()\n        self.dim = dim\n        self.depth = depth\n        self.dropout = dropout\n        self.heads = heads\n        self.d_state = d_state\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.encoder_dim = encoder_dim"
        },
        {
            "comment": "This code defines a class for a Mamba model with ViT encoder, setup ViTransformerWrapper for the encoder, linear layer to project image embeddings, VisualExpert for visual expertization, and an MLP. The dimensions are set based on input parameters.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/block.py\":70-103",
            "content": "        self.encoder_depth = encoder_depth\n        self.encoder_heads = encoder_heads\n        self.fusion_method = fusion_method\n        # Hidden dim\n        self.hidden_dim = dim * expansion_rate\n        # Set up the Mamba block\n        self.mamba = MambaBlock(\n            dim=dim, depth=depth, d_state=d_state, *args, **kwargs\n        )\n        # Set up the ViT encoder\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # Setup the linear layer to project the image embeddings to the same dimension as the text embeddings\n        self.linear = nn.Linear(encoder_dim, dim)\n        # VisualExpert\n        self.visual_expert = VisualExpert(\n            dim, self.hidden_dim, dropout, heads\n        )\n        # MLP\n        self.mlp = MLP(\n            dim, dim, expansion_factor=4, depth=1, norm=True"
        },
        {
            "comment": "This code is the forward pass of the MultiModalMambaBlock module, taking text and image as inputs and returning output embeddings after multimodal fusion. It first encodes the image using the encoder and then projects the image embeddings to the same dimension as the text embeddings. If fusion method is mlp, it uses an MLP to fuse text and image embeddings; if fusion method is concat, it concatenates text and image embeddings.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/block.py\":104-128",
            "content": "        )\n    def forward(self, text: Tensor, img: Tensor) -> Tensor:\n        \"\"\"\n        Forward pass of the MultiModalMambaBlock module.\n        Args:\n            text (Tensor): The input text embeddings.\n            img (Tensor): The input image.\n        Returns:\n            Tensor: The output embeddings after multimodal fusion.\n        \"\"\"\n        # Encode the image, Returns the same shape as text\n        encoded_img = self.encoder(img, return_embeddings=True)\n        # print(f\"Image shape: {encoded_img.shape} inside the MultiModalMambaBlock\")\n        # Project the image embeddings to the same dimension as the text embeddings\n        # We need to project the 2nd dim of the image embeddings to the same dimension as the text embeddings\n        # if the fusion method is mlp, use the mlp to fuse the text and image embeddings\n        if self.fusion_method == \"mlp\":\n            fusion_layer = self.mlp(encoded_img)\n            fused = fusion_layer + text\n        # If fusion method is concat, concatenate the text and image embeddings"
        },
        {
            "comment": "The code checks the fusion method and performs different operations based on it. If \"concat\", concatenates text and encoded_img. If \"add\", adds them. If \"visual_expert\", uses visual_expert function after concatenating both inputs. Returns result from mamba function after fusion.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/block.py\":129-143",
            "content": "        if self.fusion_method == \"concat\":\n            fused = torch.concat([text, encoded_img], dim=1)\n        if self.fusion_method == \"add\":\n            fused = encoded_img + text\n        if self.fusion_method == \"visual_expert\":\n            concat = torch.cat([text, encoded_img], dim=1)\n            fused = self.visual_expert(concat)\n        return self.mamba(fused)\n    def check_fusion_method(self):\n        print(\"\"\"[mlp] [visualexpert] [projection] [concat] [add] \"\"\")\n        print(f\"\"\"Current fusion method: {self.fusion_method}\"\"\")"
        }
    ]
}