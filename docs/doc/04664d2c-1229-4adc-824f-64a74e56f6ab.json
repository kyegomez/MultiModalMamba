{
    "summary": "The code defines a class `MMM` for multi-modal modeling with options and a MultiModalMamba model that combines text and image inputs using fusion techniques. It initializes, applies norm if needed, and returns embeddings or calculates logits.",
    "details": [
        {
            "comment": "The code defines a class MMM for the MultiModalMamba model, which takes various arguments such as vocabulary size, dimension of dense vectors, and more. It consists of layers including MLP, VisualExpert, and MambaBlock from zeta.nn, along with Encoder and ViTransformerWrapper from zeta.structs. The fusion method can be set to different options like mlp, concat, add, etc.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":0-24",
            "content": "import torch\nfrom torch import Tensor, nn\nfrom zeta import RMSNorm\nfrom zeta.nn import MLP, VisualExpert\nfrom zeta.nn.modules.simple_mamba import MambaBlock\nfrom zeta.structs import Encoder, ViTransformerWrapper\nclass MMM(nn.Module):\n    \"\"\"\n    MultiModalMamba model.\n    Args:\n        vocab_size (int): Size of the vocabulary.\n        dim (int): Dimension of the dense vectors.\n        depth (int): Number of layers in the model.\n        dropout (float): Dropout probability.\n        heads (int): Number of attention heads.\n        d_state (int): Dimension of the state.\n        image_size (int): Size of the input image.\n        patch_size (int): Size of the image patch.\n        encoder_dim (int): Dimension of the encoder.\n        encoder_depth (int): Number of layers in the encoder.\n        encoder_heads (int): Number of attention heads in the encoder.\n        fusion_method (str): Fusion method to use. Defaults to \"mlp\", can be one of \"mlp\", \"concat\", \"add\", \"visual_expert\", \"matmul\", \"mobilevlm\", \"CrossAttention\"."
        },
        {
            "comment": "This code defines a class `MMM` for multi-modal modeling, which takes various input arguments such as vocab size, model dimensions, depth, dropout rate, etc. It also includes examples on how to use the class. The class likely implements a multi-modal transformer model with options for embedding return, expansion ratio, and post-fusion normalization.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":25-66",
            "content": "        return_embeddings (bool): Whether to return the embeddings or not. Defaults to False.\n        expansion_ratio (int): Expansion ratio for the hidden dimension. Defaults to 4.\n        post_fuse_norm (bool): Whether to apply layer normalization after the fusion or not. Defaults to True.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Examples::\n    import torch\n    from mm_mamba.model import MMM\n    x = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    model = MMM(\n        vocab_size=10000,\n        dim=512,\n        depth=6,\n        dropout=0.1,\n        heads=8,\n        d_state=512,\n        image_size=224,\n        patch_size=16,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n    )\n    out = model(x, img)\n    print(out.shape)\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        dim: int,\n        depth: int,\n        dropout: float,\n        heads: int,\n        d_state: int,\n        image_size: int,"
        },
        {
            "comment": "This code defines a class \"MMM\" that extends some parent class. It takes various arguments to initialize its attributes such as patch_size, encoder_dim, encoder_depth, fusion_method, etc. It then initializes its own members including an embedding layer.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":67-98",
            "content": "        patch_size: int,\n        encoder_dim: int,\n        encoder_depth: int,\n        encoder_heads: int,\n        fusion_method: str = \"mlp\",\n        return_embeddings: bool = False,\n        expansion_ratio: int = 4,\n        post_fuse_norm: bool = True,\n        *args,\n        **kwargs,\n    ):\n        super(MMM, self).__init__()\n        self.vocab_size = vocab_size\n        self.dim = dim\n        self.depth = depth\n        self.dropout = dropout\n        self.heads = heads\n        self.d_state = d_state\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.encoder_dim = encoder_dim\n        self.encoder_depth = encoder_depth\n        self.encoder_heads = encoder_heads\n        self.fusion_method = fusion_method\n        self.return_embeddings = return_embeddings\n        self.expansion_ratio = expansion_ratio\n        self.post_fuse_norm = post_fuse_norm\n        # Transforms integer indices to dense vectors of fixed size\n        self.embedding = nn.Embedding(vocab_size, dim)\n        # MultiModalMambaBlock in a list"
        },
        {
            "comment": "This code defines a model with layers, normalization, linear layer, and projection for image. It uses the MambaBlock and ViTransformerWrapper for text and image encoding, respectively. The dimensions are defined based on input parameters.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":99-139",
            "content": "        self.layers = nn.ModuleList(\n            [\n                MambaBlock(\n                    dim,\n                    depth,\n                    d_state,\n                    expansion_ratio,\n                    *args,\n                    **kwargs,\n                )\n            ]\n        )\n        # Normalization layer\n        self.rmsnorm = RMSNorm(dim)\n        self.norm = nn.LayerNorm(dim)\n        # Linear layer\n        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n        # Tie weights\n        self.lm_head.weight = self.embedding.weight\n        # Projection for the img\n        self.img_proj = nn.Linear(dim, dim)\n        # Hidden dim\n        self.hidden_dim = dim * expansion_ratio\n        # Set up the ViT encoder\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # Setup the linear layer to project the image embeddings to the same dimension as the text embeddings"
        },
        {
            "comment": "Code defines a MultiModalMamba model with embedding, encoder, visual expert, and MLP components. The forward pass takes text and image inputs, encodes the text, encodes the image, projects the image embeddings to match text dimension, and returns output logits.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":140-170",
            "content": "        self.linear = nn.Linear(encoder_dim, dim)\n        # VisualExpert\n        self.visual_expert = VisualExpert(\n            dim, self.hidden_dim, dropout, heads\n        )\n        # MLP\n        self.mlp = MLP(\n            dim, dim, expansion_factor=4, depth=1, norm=True\n        )\n    def forward(self, text: Tensor, img: Tensor) -> Tensor:\n        \"\"\"\n        Forward pass of the MultiModalMamba model.\n        Args:\n            text (Tensor): Input text tensor.\n            img (Tensor): Input image tensor.\n        Returns:\n            Tensor: Output logits.\n        \"\"\"\n        x = self.embedding(text)\n        # print(f\"Text shape: {x.shape} inside the MMM\")\n        # Encode the image, Returns the same shape as text\n        encoded_img = self.encoder(img, return_embeddings=True)\n        # print(f\"Image shape: {encoded_img.shape} inside the MMM\")\n        # Project the image embeddings to the same dimension as the text embeddings\n        # We need to project the 2nd dim of the image embeddings to the same dimension as the text embeddings"
        },
        {
            "comment": "This code implements a fusion method for combining text and image embeddings in a multimodal model. It supports several fusion techniques: MLP, concatenation, addition, visual expert, and matrix multiplication. If the post-fusion normalization is enabled, the fused representation is normalized.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":172-201",
            "content": "        # if the fusion method is mlp, use the mlp to fuse the text and image embeddings\n        if self.fusion_method == \"mlp\":\n            fusion_layer = self.mlp(encoded_img)\n            fused = fusion_layer + x\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        # If fusion method is concat, concatenate the text and image embeddings\n        if self.fusion_method == \"concat\":\n            fused = torch.concat([x, encoded_img], dim=1)\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"add\":\n            fused = encoded_img + x\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"visual_expert\":\n            concat = torch.cat([x, encoded_img], dim=1)\n            fused = self.visual_expert(concat)\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"matmul\":\n            fused = torch.matmul(encoded_img, x)"
        },
        {
            "comment": "This code segment initializes a fused representation and applies norm if post_fuse_norm is True. It needs to implement fusion methods \"mobilevlm\" and \"CrossAttention\". For each layer in layers, it passes the input through the layer and adds it to the previous result. If return_embeddings is True, it returns the final embedding; otherwise, it applies norm and feeds the output into lm_head for logits calculation.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/mm_mamba/model.py\":203-225",
            "content": "            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        # Need to implement this\n        if self.fusion_method == \"mobilevlm\":\n            pass\n        # Need to implement this\n        if self.fusion_method == \"CrossAttention\":\n            pass\n        x = fused\n        for layer in self.layers:\n            x = layer(x) + x\n        if self.return_embeddings:\n            return x\n        else:\n            x = self.norm(x)\n            logits = self.lm_head(x)\n            return logits"
        }
    ]
}