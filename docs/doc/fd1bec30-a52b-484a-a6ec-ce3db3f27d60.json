{
    "summary": "This code produces professional documentation in markdown format and offers a function for multi-head attention computation, discussing robust Python test best practices and pytest optimization.",
    "details": [
        {
            "comment": "This function generates a multi-page, long, and explicit professional documentation for a given module or framework using markdown format. It requires the task (module) name and the module name as inputs. The generated documentation will include clear explanations, usage examples, tables for arguments and methods, and a thorough understanding of the code's architecture and purpose.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":0-17",
            "content": "def DOCUMENTATION_WRITER_SOP(\n    task: str,\n    module: str,\n):\n    documentation = f\"\"\"Create multi-page long and explicit professional pytorch-like documentation for the {module} code below follow the outline for the {module} library,\n    provide many examples and teach the user about the code, provide examples for every function, make the documentation 10,000 words,\n    provide many usage examples and note this is markdown docs, create the documentation for the code to document,\n    put the arguments and methods in a table in markdown to make it visually seamless\n    Now make the professional documentation for this code, provide the architecture and how the class works and why it works that way,\n    it's purpose, provide args, their types, 3 ways of usage examples, in examples show all the code like imports main example etc\n    BE VERY EXPLICIT AND THOROUGH, MAKE IT DEEP AND USEFUL\n    ########\n    Step 1: Understand the purpose and functionality of the module or framework\n    Read and analyze "
        },
        {
            "comment": "This code outlines the steps to write effective documentation for a module or framework. It suggests providing an overview and introduction, defining classes/functions with their parameters, explaining functionality and usage, and highlighting key concepts or terminology.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":17-31",
            "content": "the description provided in the documentation to understand the purpose and functionality of the module or framework.\n    Identify the key features, parameters, and operations performed by the module or framework.\n    Step 2: Provide an overview and introduction\n    Start the documentation by providing a brief overview and introduction to the module or framework.\n    Explain the importance and relevance of the module or framework in the context of the problem it solves.\n    Highlight any key concepts or terminology that will be used throughout the documentation.\n    Step 3: Provide a class or function definition\n    Provide the class or function definition for the module or framework.\n    Include the parameters that need to be passed to the class or function and provide a brief description of each parameter.\n    Specify the data types and default values for each parameter.\n    Step 4: Explain the functionality and usage\n    Provide a detailed explanation of how the module or framework works and what it does."
        },
        {
            "comment": "This code provides a template for documenting a module or function, including steps for explaining its usage, providing examples, outlining inputs and outputs, offering additional tips, addressing common issues, and including references.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":32-47",
            "content": "    Describe the steps involved in using the module or framework, including any specific requirements or considerations.\n    Provide code examples to demonstrate the usage of the module or framework.\n    Explain the expected inputs and outputs for each operation or function.\n    Step 5: Provide additional information and tips\n    Provide any additional information or tips that may be useful for using the module or framework effectively.\n    Address any common issues or challenges that developers may encounter and provide recommendations or workarounds.\n    Step 6: Include references and resources\n    Include references to any external resources or research papers that provide further information or background on the module or framework.\n    Provide links to relevant documentation or websites for further exploration.\n    Example Template for the given documentation:\n    # Module/Function Name: MultiheadAttention\n    class torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, ad"
        },
        {
            "comment": "This code snippet defines a function for creating a multi-head attention module. It takes parameters such as embed_dim, num_heads, dropout, and more to create an information representation from different subspaces. The module includes options for adding bias or zero attention to the key and value sequences.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":47-59",
            "content": "d_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None):\n        ```\n        Creates a multi-head attention module for joint information representation from the different subspaces.\n        Parameters:\n        - embed_dim (int): Total dimension of the model.\n        - num_heads (int): Number of parallel attention heads. The embed_dim will be split across num_heads.\n        - dropout (float): Dropout probability on attn_output_weights. Default: 0.0 (no dropout).\n        - bias (bool): If specified, adds bias to input/output projection layers. Default: True.\n        - add_bias_kv (bool): If specified, adds bias to the key and value sequences at dim=0. Default: False.\n        - add_zero_attn (bool): If specified, adds a new batch of zeros to the key and value sequences at dim=1. Default: False.\n        - kdim (int): Total number of features for keys. Default: None (uses kdim=embed_dim).\n        - vdim (int): Total number of features for values. Default: None (uses vdim=embed_dim)."
        },
        {
            "comment": "This function defines the forward pass for a multi-head attention module. It takes query, key, and value tensors as input along with optional parameters such as batch_first, device, and dtype. The function performs the attention mechanism to calculate the output based on the given inputs and optional parameters.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":60-72",
            "content": "        - batch_first (bool): If True, the input and output tensors are provided as (batch, seq, feature). Default: False.\n        - device (torch.device): If specified, the tensors will be moved to the specified device.\n        - dtype (torch.dtype): If specified, the tensors will have the specified dtype.\n        ```\n        def forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False):\n            ```\n            Forward pass of the multi-head attention module.\n            Parameters:\n            - query (Tensor): Query embeddings of shape (L, E_q) for unbatched input, (L, N, E_q) when batch_first=False, or (N, L, E_q) when batch_first=True.\n            - key (Tensor): Key embeddings of shape (S, E_k) for unbatched input, (S, N, E_k) when batch_first=False, or (N, S, E_k) when batch_first=True.\n            - value (Tensor): Value embeddings of shape (S, E_v) for unbatched input, (S, N, E_v) when batch_first=False, or (N, S, E_v) when batch_first=True."
        },
        {
            "comment": "This code defines the function signature for a multi-head attention operation. It accepts parameters such as key_padding_mask, need_weights, attn_mask, average_attn_weights and is_causal, and returns the attention output and optionally the attention weights. The return type is a tuple of Tensors, where the first element is the attn_output with shape (L, E), (L, N, E) or (N, L, E) depending on batch_first parameter, and the second optional element is the attn_output_weights Tensor if need_weights is True. The average_attn_weights flag affects whether the attention weights are averaged or returned separately per head. The is_causal flag applies a causal mask to the attention computation.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":73-82",
            "content": "            - key_padding_mask (Optional[Tensor]): If specified, a mask indicating elements to be ignored in key for attention computation.\n            - need_weights (bool): If specified, returns attention weights in addition to attention outputs. Default: True.\n            - attn_mask (Optional[Tensor]): If specified, a mask preventing attention to certain positions.\n            - average_attn_weights (bool): If true, returns averaged attention weights per head. Otherwise, returns attention weights separately per head. Note that this flag only has an effect when need_weights=True. Default: True.\n            - is_causal (bool): If specified, applies a causal mask as the attention mask. Default: False.\n            Returns:\n            Tuple[Tensor, Optional[Tensor]]:\n            - attn_output (Tensor): Attention outputs of shape (L, E) for unbatched input, (L, N, E) when batch_first=False, or (N, L, E) when batch_first=True.\n            - attn_output_weights (Optional[Tensor]): Attention weigh"
        },
        {
            "comment": "This code defines a function `TEST_WRITER_SOP_PROMPT` that takes in a task, module name, and path. It generates a prompt for creating 5000 lines of data using the specified task and module, with optional additional arguments. The MultiheadAttention class is defined as well, which performs a forward pass on input queries, keys, and values and returns the output and weights if need_weights is True.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":82-111",
            "content": "ts of shape (L, S) when unbatched or (N, L, S) when batched. Optional, only returned when need_weights=True.\n            ```\n            # Implementation of the forward pass of the attention module goes here\n            return attn_output, attn_output_weights\n            ```\n            # Usage example:\n            multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n            attn_output, attn_output_weights = multihead_attn(query, key, value)\n            Note:\n            The above template includes the class or function definition, parameters, description, and usage example.\n            To replicate the documentation for any other module or framework, follow the same structure and provide the specific details for that module or framework.\n            ############# DOCUMENT THE FOLLOWING CODE ########\n            {task}\n            \"\"\"\n    return documentation\ndef TEST_WRITER_SOP_PROMPT(\n    task: str, module: str, path: str, *args, **kwargs\n):\n    TESTS_PROMPT = f\"\"\"\n   Create 5,000 lines of"
        },
        {
            "comment": "This code is a testing guide for using pytest to create extensive, thorough, and production-ready tests. It covers preparation steps like installing pytest and structuring the project, writing basic tests with clear function names, utilizing fixtures, and parameterized testing.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":111-134",
            "content": " extensive and thorough tests for the code below using the guide, do not worry about your limits you do not have any\n   just write the best tests possible, the module is {module}, the file path is {path}\n   ######### TESTING GUIDE #############\n   # **Guide to Creating Extensive, Thorough, and Production-Ready Tests using `pytest`**\n   1. **Preparation**:\n      - Install pytest: `pip install pytest`.\n      - Structure your project so that tests are in a separate `tests/` directory.\n      - Name your test files with the prefix `test_` for pytest to recognize them.\n   2. **Writing Basic Tests**:\n      - Use clear function names prefixed with `test_` (e.g., `test_check_value()`).\n      - Use assert statements to validate results.\n   3. **Utilize Fixtures**:\n      - Fixtures are a powerful feature to set up preconditions for your tests.\n      - Use `@pytest.fixture` decorator to define a fixture.\n      - Pass fixture name as an argument to your test to use it.\n   4. **Parameterized Testing**:\n      - Use `@pytest.mark.parametrize` to run a test multiple times with different inputs."
        },
        {
            "comment": "The code provides guidelines for writing robust tests in Python. It suggests using parametrized testing, mocking and monkeypatching to isolate units of code, exception handling, and coverage reports. Additionally, it advises on managing environment variables and secret handling, grouping and marking tests, all aimed at improving test quality and efficiency.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":135-155",
            "content": "      - This helps in thorough testing with various input values without writing redundant code.\n   5. **Use Mocks and Monkeypatching**:\n      - Use `monkeypatch` fixture to modify or replace classes/functions during testing.\n      - Use `unittest.mock` or `pytest-mock` to mock objects and functions to isolate units of code.\n   6. **Exception Testing**:\n      - Test for expected exceptions using `pytest.raises(ExceptionType)`.\n   7. **Test Coverage**:\n      - Install pytest-cov: `pip install pytest-cov`.\n      - Run tests with `pytest --cov=my_module` to get a coverage report.\n   8. **Environment Variables and Secret Handling**:\n      - Store secrets and configurations in environment variables.\n      - Use libraries like `python-decouple` or `python-dotenv` to load environment variables.\n      - For tests, mock or set environment variables temporarily within the test environment.\n   9. **Grouping and Marking Tests**:\n      - Use `@pytest.mark` decorator to mark tests (e.g., `@pytest.mark.slow`).\n      - This allows for selectively running certain groups of tests."
        },
        {
            "comment": "This code snippet provides tips for optimizing pytest usage, including leveraging plugins, integrating with CI platforms, and implementing logging and reporting tools. It also covers best practices for handling databases, concurrency issues, and maintaining clean code.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":157-177",
            "content": "   10. **Use Plugins**:\n      - Utilize the rich ecosystem of pytest plugins (e.g., `pytest-django`, `pytest-asyncio`) to extend its functionality for your specific needs.\n   11. **Continuous Integration (CI)**:\n      - Integrate your tests with CI platforms like Jenkins, Travis CI, or GitHub Actions.\n      - Ensure tests are run automatically with every code push or pull request.\n   12. **Logging and Reporting**:\n      - Use `pytest`'s inbuilt logging.\n      - Integrate with tools like `Allure` for more comprehensive reporting.\n   13. **Database and State Handling**:\n      - If testing with databases, use database fixtures or factories to create a known state before tests.\n      - Clean up and reset state post-tests to maintain consistency.\n   14. **Concurrency Issues**:\n      - Consider using `pytest-xdist` for parallel test execution.\n      - Always be cautious when testing concurrent code to avoid race conditions.\n   15. **Clean Code Practices**:\n      - Ensure tests are readable and maintainable."
        },
        {
            "comment": "The code snippet provides a guide for creating effective tests by focusing on functionality, regular maintenance, documentation, and feedback loop. It emphasizes the importance of adapting and expanding upon these guidelines based on specific project requirements.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/scripts/auto_tests_docs/docs.py\":178-200",
            "content": "      - Avoid testing implementation details; focus on functionality and expected behavior.\n   16. **Regular Maintenance**:\n      - Periodically review and update tests.\n      - Ensure that tests stay relevant as your codebase grows and changes.\n   17. **Documentation**:\n      - Document test cases, especially for complex functionalities.\n      - Ensure that other developers can understand the purpose and context of each test.\n   18. **Feedback Loop**:\n      - Use test failures as feedback for development.\n      - Continuously refine tests based on code changes, bug discoveries, and additional requirements.\n   By following this guide, your tests will be thorough, maintainable, and production-ready. Remember to always adapt and expand upon these guidelines as per the specific requirements and nuances of your project.\n   ######### CREATE TESTS FOR THIS CODE: #######\n   {task}\n   \"\"\"\n    return TESTS_PROMPT"
        }
    ]
}