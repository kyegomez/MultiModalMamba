{
    "summary": "This code defines a `MMM` model test fixture and tests its forward pass behavior with different fusion methods, considering the `return_embeddings` flag.",
    "details": [
        {
            "comment": "This code defines a test fixture function `mmm()` that returns an instance of the `MMM` model with specific hyperparameters. The tests ensure the model is properly initialized and check if the model's attributes match the expected values. Additionally, it checks the shape of the output from the forward pass of the model when given input text and image data.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/tests/test_model.py\":0-45",
            "content": "import torch\nimport pytest\nfrom mm_mamba.model import MMM\n@pytest.fixture\ndef mmm():\n    return MMM(\n        vocab_size=10000,\n        dim=512,\n        depth=6,\n        dropout=0.1,\n        heads=8,\n        d_state=512,\n        image_size=224,\n        patch_size=16,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n        fusion_method=\"mlp\",\n        return_embeddings=False,\n    )\ndef test_mmm_initialization(mmm):\n    assert isinstance(mmm, MMM)\n    assert mmm.vocab_size == 10000\n    assert mmm.dim == 512\n    assert mmm.depth == 6\n    assert mmm.dropout == 0.1\n    assert mmm.heads == 8\n    assert mmm.d_state == 512\n    assert mmm.image_size == 224\n    assert mmm.patch_size == 16\n    assert mmm.encoder_dim == 512\n    assert mmm.encoder_depth == 6\n    assert mmm.encoder_heads == 8\n    assert mmm.fusion_method == \"mlp\"\n    assert mmm.return_embeddings is False\ndef test_mmm_forward(mmm):\n    text = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    out = mmm(text, img)\n    assert out.shape == (1, 224, 10000)"
        },
        {
            "comment": "This code defines and tests a function `test_mmm_forward_with_different_fusion_methods_2` that checks the behavior of the MultiModalMamba model with various fusion methods, considering the `return_embeddings` flag. It initializes a Model instance, sets its `fusion_method`, generates random text and image tensors, calls the model's forward method, and asserts the output shape based on whether `return_embeddings` is True or False.",
            "location": "\"/media/root/Prima/works/MultiModalMamba/docs/src/tests/test_model.py\":48-83",
            "content": "def test_mmm_return_embeddings(mmm):\n    mmm.return_embeddings = True\n    text = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    out = mmm(text, img)\n    assert out.shape == (1, 224, 512)\n@pytest.mark.parametrize(\n    \"fusion_method\", [\"mlp\", \"concat\", \"add\", \"visual_expert\"]\n)\ndef test_mmm_forward_with_different_fusion_methods(\n    mmm, fusion_method\n):\n    mmm.fusion_method = fusion_method\n    text = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    out = mmm(text, img)\n    assert out.shape == (1, 224, 10000)\n@pytest.mark.parametrize(\n    \"fusion_method\", [\"mlp\", \"concat\", \"add\", \"visual_expert\"]\n)\ndef test_mmm_forward_with_different_fusion_methods_2(\n    mmm, fusion_method\n):\n    mmm.fusion_method = fusion_method\n    text = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    out = mmm(text, img)\n    assert (\n        out.shape == (1, 224, 10000)\n        if not mmm.return_embeddings\n        else (1, 224, 512)\n    )"
        }
    ]
}