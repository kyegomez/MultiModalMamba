{
    "0": {
        "file_id": 0,
        "content": "/README.md",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "MultiModalMamba (MMM) is a versatile, efficient AI model combining Vision Transformer and Mamba for text-image processing. Built on Zeta, it outperforms GPT-4 and LLAMA in speed and customization with MIT license.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n# Multi Modal Mamba - [MMM]\nMulti Modal Mamba (MMM) is an all-new AI model that integrates Vision Transformer (ViT) and Mamba, creating a high-performance multi-modal model. MMM is built on Zeta, a minimalist yet powerful AI framework, designed to streamline and enhance machine learning model management. \nThe capacity to process and interpret multiple data types concurrently is essential, the world isn't 1dimensional. MMM addresses this need by leveraging the capabilities of Vision Transformer and Mamba, enabling efficient handling of both text and image data. This makes MMM a versatile solution for a broad spectrum of AI tasks. MMM stands out for its significant speed and efficiency improvements over traditional transformer architectures, such as GPT-4 and LLAMA. This enhancement allows MMM to deliver high-quality results without sacrificing performance, making it an optimal choice for real-time data processing and compl",
        "type": "code",
        "location": "/README.md:1-6"
    },
    "3": {
        "file_id": 0,
        "content": "Multi Modal Mamba (MMM) is a new AI model integrating Vision Transformer (ViT) and Mamba, built on Zeta for efficient multi-modal processing. It excels in text and image data handling and outperforms traditional architectures like GPT-4 and LLAMA with speed and efficiency.",
        "type": "comment"
    },
    "4": {
        "file_id": 0,
        "content": "ex AI algorithm execution. A key feature of MMM is its proficiency in processing extremely long sequences.\nThis capability is particularly beneficial for tasks that involve substantial data volumes or necessitate a comprehensive understanding of context, such as natural language processing or image recognition. With MMM, you're not just adopting a state-of-the-art AI model. You're integrating a fast, efficient, and robust tool that is equipped to meet the demands of contemporary AI tasks. Experience the power and versatility of Multi Modal Mamba - MMM now!\n## Install\n`pip3 install mmm-zeta`\n## Usage\n### `MultiModalMambaBlock`\n```python\n# Import the necessary libraries\nimport torch \nfrom torch import nn\nfrom mm_mamba import MultiModalMambaBlock\n# Create some random input tensors\nx = torch.randn(1, 16, 64)  # Tensor with shape (batch_size, sequence_length, feature_dim)\ny = torch.randn(1, 3, 64, 64)  # Tensor with shape (batch_size, num_channels, image_height, image_width)\n# Create an instance of the MultiModalMambaBlock model",
        "type": "code",
        "location": "/README.md:6-29"
    },
    "5": {
        "file_id": 0,
        "content": "This code installs the MultiModalMamba (MMM) AI model using pip3 and demonstrates its usage by importing necessary libraries, creating random input tensors, and instantiating an instance of the MultiModalMambaBlock model. MMM is a state-of-the-art model proficient in processing long sequences, making it beneficial for tasks involving large data volumes or comprehensive context understanding like natural language processing and image recognition.",
        "type": "comment"
    },
    "6": {
        "file_id": 0,
        "content": "model = MultiModalMambaBlock(\n    dim = 64,  # Dimension of the token embeddings\n    depth = 5,  # Number of transformer layers\n    dropout = 0.1,  # Dropout probability\n    heads = 4,  # Number of attention heads\n    d_state = 16,  # Dimension of the state embeddings\n    image_size = 64,  # Size of the input image\n    patch_size = 16,  # Size of each image patch\n    encoder_dim = 64,  # Dimension of the encoder token embeddings\n    encoder_depth = 5,  # Number of encoder transformer layers\n    encoder_heads = 4,  # Number of encoder attention heads\n    fusion_method=\"mlp\",\n)\n# Pass the input tensors through the model\nout = model(x, y)\n# Print the shape of the output tensor\nprint(out.shape)\n```\n### `MMM`, Ready to Train Model\n- Flexibility in Data Types: The MMM model can handle both text and image data simultaneously. This allows it to be trained on a wider variety of datasets and tasks, including those that require understanding of both text and image data.\n- Customizable Architecture: The MMM model has",
        "type": "code",
        "location": "/README.md:30-56"
    },
    "7": {
        "file_id": 0,
        "content": "This code creates an instance of the MultiModalMambaBlock model, which is a transformer-based architecture designed to handle both text and image data. The model's parameters such as dimension, depth, dropout probability, number of heads, dimension of state embeddings, size of input image, and patch size are customizable for flexibility. Once created, the model can be passed input tensors x and y, and the output tensor's shape is printed.",
        "type": "comment"
    },
    "8": {
        "file_id": 0,
        "content": " numerous parameters such as depth, dropout, heads, d_state, image_size, patch_size, encoder_dim, encoder_depth, encoder_heads, and fusion_method. These parameters can be tuned according to the specific requirements of the task at hand, allowing for a high degree of customization in the model architecture.\n- Option to Return Embeddings: The MMM model has a return_embeddings option. When set to True, the model will return the embeddings instead of the final output. This can be useful for tasks that require access to the intermediate representations learned by the model, such as transfer learning or feature extraction tasks.\n```python\nimport torch  # Import the torch library\n# Import the MMM model from the mm_mamba module\nfrom mm_mamba import MMM\n# Generate a random tensor 'x' of size (1, 224) with random elements between 0 and 10000\nx = torch.randint(0, 10000, (1, 196))\n# Generate a random image tensor 'img' of size (1, 3, 224, 224)\nimg = torch.randn(1, 3, 224, 224)\n# Create a MMM model object with the following parameters:",
        "type": "code",
        "location": "/README.md:56-72"
    },
    "9": {
        "file_id": 0,
        "content": "The code snippet is importing the MMM model from the mm_mamba module and creating a tensor 'x' with random elements between 0 and 10000. It also creates a random image tensor 'img'. The code then creates an instance of the MMM model object with the specified parameters, which can be tuned for specific task requirements.",
        "type": "comment"
    },
    "10": {
        "file_id": 0,
        "content": "model = MMM(\n    vocab_size=10000,\n    dim=512,\n    depth=6,\n    dropout=0.1,\n    heads=8,\n    d_state=512,\n    image_size=224,\n    patch_size=16,\n    encoder_dim=512,\n    encoder_depth=6,\n    encoder_heads=8,\n    fusion_method=\"mlp\",\n    return_embeddings=False,\n    post_fuse_norm=True,\n)\n# Pass the tensor 'x' and 'img' through the model and store the output in 'out'\nout = model(x, img)\n# Print the shape of the output tensor 'out'\nprint(out.shape)\n# After much training\nmodel.eval()\n# Tokenize texts\ntext_tokens = tokenize(text)\n# Send text tokens to the model\nlogits = model(text_tokens)\ntext = detokenize(logits)\n```\n# Real-World Deployment\nAre you an enterprise looking to leverage the power of AI? Do you want to integrate state-of-the-art models into your workflow? Look no further!\nMulti Modal Mamba (MMM) is a cutting-edge AI model that fuses Vision Transformer (ViT) with Mamba, providing a fast, agile, and high-performance solution for your multi-modal needs. \nBut that's not all! With Zeta, our simple yet powerful ",
        "type": "code",
        "location": "/README.md:73-115"
    },
    "11": {
        "file_id": 0,
        "content": "The code initializes a MultiModalMamba model with specific parameters, passes input tensors 'x' and 'img' through the model to obtain output tensor 'out', prints the shape of 'out', sets the model to evaluation mode, tokenizes a text using 'tokenize()', sends text tokens to the model to get logits, and finally detokenizes logits to get original text.",
        "type": "comment"
    },
    "12": {
        "file_id": 0,
        "content": "AI framework, you can easily customize and fine-tune MMM to perfectly fit your unique quality standards. \nWhether you're dealing with text, images, or both, MMM has got you covered. With its deep configuration and multiple fusion layers, you can handle complex AI tasks with ease and efficiency.\n### :star2: Why Choose Multi Modal Mamba?\n- **Versatile**: Handle both text and image data with a single model.\n- **Powerful**: Leverage the power of Vision Transformer and Mamba.\n- **Customizable**: Fine-tune the model to your specific needs with Zeta.\n- **Efficient**: Achieve high performance without compromising on speed.\nDon't let the complexities of AI slow you down. Choose Multi Modal Mamba and stay ahead of the curve!\n[Contact us here](https://calendly.com/swarm-corp/30min) today to learn how you can integrate Multi Modal Mamba into your workflow and supercharge your AI capabilities!\n---\n# License\nMIT",
        "type": "code",
        "location": "/README.md:115-134"
    },
    "13": {
        "file_id": 0,
        "content": "The code is a description of Multi Modal Mamba, an AI framework that can handle both text and image data, leveraging Vision Transformer and Mamba while being versatile, powerful, customizable, and efficient. It encourages users to contact for integration and learning more about it under the MIT license.",
        "type": "comment"
    },
    "14": {
        "file_id": 1,
        "content": "/example.py",
        "type": "filepath"
    },
    "15": {
        "file_id": 1,
        "content": "The code imports libraries, creates input tensors, initializes a MultiModalMambaBlock model, passes tensors through the model, and prints the output tensor's shape.",
        "type": "summary"
    },
    "16": {
        "file_id": 1,
        "content": "# Import the necessary libraries\nimport torch\nfrom mm_mamba import MultiModalMambaBlock\n# Create some random input tensors\nx = torch.randn(\n    1, 16, 64\n)  # Tensor with shape (batch_size, sequence_length, feature_dim)\ny = torch.randn(\n    1, 3, 64, 64\n)  # Tensor with shape (batch_size, num_channels, image_height, image_width)\n# Create an instance of the MultiModalMambaBlock model\nmodel = MultiModalMambaBlock(\n    dim=64,  # Dimension of the token embeddings\n    depth=5,  # Number of transformer layers\n    dropout=0.1,  # Dropout probability\n    heads=4,  # Number of attention heads\n    d_state=16,  # Dimension of the state embeddings\n    image_size=64,  # Size of the input image\n    patch_size=16,  # Size of each image patch\n    encoder_dim=64,  # Dimension of the encoder token embeddings\n    encoder_depth=5,  # Number of encoder transformer layers\n    encoder_heads=4,  # Number of encoder attention heads\n    fusion_method=\"mlp\",\n)\n# Pass the input tensors through the model\nout = model(x, y)\n# Print the shape of the output tensor",
        "type": "code",
        "location": "/example.py:1-31"
    },
    "17": {
        "file_id": 1,
        "content": "Code imports necessary libraries, creates random input tensors, initializes a MultiModalMambaBlock model with specified dimensions and parameters, passes the input tensors through the model, and prints the shape of the output tensor.",
        "type": "comment"
    },
    "18": {
        "file_id": 1,
        "content": "print(out.shape)",
        "type": "code",
        "location": "/example.py:32-32"
    },
    "19": {
        "file_id": 1,
        "content": "This line of code prints the shape (dimensions) of the variable 'out'.",
        "type": "comment"
    },
    "20": {
        "file_id": 2,
        "content": "/mm_mamba/__init__.py",
        "type": "filepath"
    },
    "21": {
        "file_id": 2,
        "content": "This code imports the MultiModalMambaBlock and MMM classes from their respective modules, and adds them to the __all__ list for easy access.",
        "type": "summary"
    },
    "22": {
        "file_id": 2,
        "content": "from mm_mamba.block import MultiModalMambaBlock\nfrom mm_mamba.model import MMM\n__all__ = [\"MultiModalMambaBlock\", \"MMM\"]",
        "type": "code",
        "location": "/mm_mamba/__init__.py:1-4"
    },
    "23": {
        "file_id": 2,
        "content": "This code imports the MultiModalMambaBlock and MMM classes from their respective modules, and adds them to the __all__ list for easy access.",
        "type": "comment"
    },
    "24": {
        "file_id": 3,
        "content": "/mm_mamba/block.py",
        "type": "filepath"
    },
    "25": {
        "file_id": 3,
        "content": "MultiModalMambaBlock is a PyTorch module for multimodal data fusion. It utilizes an encoder-decoder architecture, supports various fusion methods and defines the Mamba model class with ViT encoder. The code performs different operations based on the selected fusion method.",
        "type": "summary"
    },
    "26": {
        "file_id": 3,
        "content": "import torch\nfrom torch import nn, Tensor\nfrom zeta.nn import VisualExpert, MLP\nfrom zeta.nn.modules.simple_mamba import MambaBlock\nfrom zeta.structs import ViTransformerWrapper, Encoder\nclass MultiModalMambaBlock(nn.Module):\n    \"\"\"\n    MultiModalMambaBlock is a PyTorch module that combines text and image embeddings using a multimodal fusion approach.\n    Args:\n        dim (int): The dimension of the embeddings.\n        depth (int): The depth of the Mamba block.\n        dropout (float): The dropout rate.\n        heads (int): The number of attention heads.\n        d_state (int): The dimension of the state in the Mamba block.\n        image_size (int): The size of the input image.\n        patch_size (int): The size of the image patches.\n        encoder_dim (int): The dimension of the encoder embeddings.\n        encoder_depth (int): The depth of the encoder.\n        encoder_heads (int): The number of attention heads in the encoder.\n        fusion_method (str): The multimodal fusion method to use. Can be one of [\"mlp\", \"concat\", \"add\"].",
        "type": "code",
        "location": "/mm_mamba/block.py:1-23"
    },
    "27": {
        "file_id": 3,
        "content": "The MultiModalMambaBlock is a PyTorch module that combines text and image embeddings using a multimodal fusion approach. It takes arguments such as dimension of embeddings, depth, dropout rate, number of attention heads, and other parameters for the Mamba block and encoder. The fusion method can be set to \"mlp\", \"concat\", or \"add\".",
        "type": "comment"
    },
    "28": {
        "file_id": 3,
        "content": "    Examples:\n    x = torch.randn(1, 16, 64)\n    y = torch.randn(1, 3, 64, 64)\n    model = MultiModalMambaBlock(\n        dim = 64,\n        depth = 5,\n        dropout = 0.1,\n        heads = 4,\n        d_state = 16,\n        image_size = 64,\n        patch_size = 16,\n        encoder_dim = 64,\n        encoder_depth = 5,\n        encoder_heads = 4\n    )\n    out = model(x, y)\n    print(out.shape)\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        dropout: float,\n        heads: int,\n        d_state: int,\n        image_size: int,\n        patch_size: int,\n        encoder_dim: int,\n        encoder_depth: int,\n        encoder_heads: int,\n        fusion_method: str = \"mlp\",\n        expansion_rate: int = 2,\n        *args,\n        **kwargs,\n    ):\n        super(MultiModalMambaBlock, self).__init__()\n        self.dim = dim\n        self.depth = depth\n        self.dropout = dropout\n        self.heads = heads\n        self.d_state = d_state\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.encoder_dim = encoder_dim",
        "type": "code",
        "location": "/mm_mamba/block.py:25-70"
    },
    "29": {
        "file_id": 3,
        "content": "This code defines a class called MultiModalMambaBlock, which is an encoder-decoder based architecture for multimodal data. It takes various parameters including dimensions, depth, dropout rate, number of heads, state dimension, image size, and patch size. It also accepts optional fusion method and expansion rate parameters.",
        "type": "comment"
    },
    "30": {
        "file_id": 3,
        "content": "        self.encoder_depth = encoder_depth\n        self.encoder_heads = encoder_heads\n        self.fusion_method = fusion_method\n        # Hidden dim\n        self.hidden_dim = dim * expansion_rate\n        # Set up the Mamba block\n        self.mamba = MambaBlock(\n            dim=dim, depth=depth, d_state=d_state, *args, **kwargs\n        )\n        # Set up the ViT encoder\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # Setup the linear layer to project the image embeddings to the same dimension as the text embeddings\n        self.linear = nn.Linear(encoder_dim, dim)\n        # VisualExpert\n        self.visual_expert = VisualExpert(\n            dim, self.hidden_dim, dropout, heads\n        )\n        # MLP\n        self.mlp = MLP(\n            dim, dim, expansion_factor=4, depth=1, norm=True",
        "type": "code",
        "location": "/mm_mamba/block.py:71-104"
    },
    "31": {
        "file_id": 3,
        "content": "This code defines a class for a Mamba model with ViT encoder, setup ViTransformerWrapper for the encoder, linear layer to project image embeddings, VisualExpert for visual expertization, and an MLP. The dimensions are set based on input parameters.",
        "type": "comment"
    },
    "32": {
        "file_id": 3,
        "content": "        )\n    def forward(self, text: Tensor, img: Tensor) -> Tensor:\n        \"\"\"\n        Forward pass of the MultiModalMambaBlock module.\n        Args:\n            text (Tensor): The input text embeddings.\n            img (Tensor): The input image.\n        Returns:\n            Tensor: The output embeddings after multimodal fusion.\n        \"\"\"\n        # Encode the image, Returns the same shape as text\n        encoded_img = self.encoder(img, return_embeddings=True)\n        # print(f\"Image shape: {encoded_img.shape} inside the MultiModalMambaBlock\")\n        # Project the image embeddings to the same dimension as the text embeddings\n        # We need to project the 2nd dim of the image embeddings to the same dimension as the text embeddings\n        # if the fusion method is mlp, use the mlp to fuse the text and image embeddings\n        if self.fusion_method == \"mlp\":\n            fusion_layer = self.mlp(encoded_img)\n            fused = fusion_layer + text\n        # If fusion method is concat, concatenate the text and image embeddings",
        "type": "code",
        "location": "/mm_mamba/block.py:105-129"
    },
    "33": {
        "file_id": 3,
        "content": "This code is the forward pass of the MultiModalMambaBlock module, taking text and image as inputs and returning output embeddings after multimodal fusion. It first encodes the image using the encoder and then projects the image embeddings to the same dimension as the text embeddings. If fusion method is mlp, it uses an MLP to fuse text and image embeddings; if fusion method is concat, it concatenates text and image embeddings.",
        "type": "comment"
    },
    "34": {
        "file_id": 3,
        "content": "        if self.fusion_method == \"concat\":\n            fused = torch.concat([text, encoded_img], dim=1)\n        if self.fusion_method == \"add\":\n            fused = encoded_img + text\n        if self.fusion_method == \"visual_expert\":\n            concat = torch.cat([text, encoded_img], dim=1)\n            fused = self.visual_expert(concat)\n        return self.mamba(fused)\n    def check_fusion_method(self):\n        print(\"\"\"[mlp] [visualexpert] [projection] [concat] [add] \"\"\")\n        print(f\"\"\"Current fusion method: {self.fusion_method}\"\"\")",
        "type": "code",
        "location": "/mm_mamba/block.py:130-144"
    },
    "35": {
        "file_id": 3,
        "content": "The code checks the fusion method and performs different operations based on it. If \"concat\", concatenates text and encoded_img. If \"add\", adds them. If \"visual_expert\", uses visual_expert function after concatenating both inputs. Returns result from mamba function after fusion.",
        "type": "comment"
    },
    "36": {
        "file_id": 4,
        "content": "/mm_mamba/model.py",
        "type": "filepath"
    },
    "37": {
        "file_id": 4,
        "content": "The code defines a class `MMM` for multi-modal modeling with options and a MultiModalMamba model that combines text and image inputs using fusion techniques. It initializes, applies norm if needed, and returns embeddings or calculates logits.",
        "type": "summary"
    },
    "38": {
        "file_id": 4,
        "content": "import torch\nfrom torch import Tensor, nn\nfrom zeta import RMSNorm\nfrom zeta.nn import MLP, VisualExpert\nfrom zeta.nn.modules.simple_mamba import MambaBlock\nfrom zeta.structs import Encoder, ViTransformerWrapper\nclass MMM(nn.Module):\n    \"\"\"\n    MultiModalMamba model.\n    Args:\n        vocab_size (int): Size of the vocabulary.\n        dim (int): Dimension of the dense vectors.\n        depth (int): Number of layers in the model.\n        dropout (float): Dropout probability.\n        heads (int): Number of attention heads.\n        d_state (int): Dimension of the state.\n        image_size (int): Size of the input image.\n        patch_size (int): Size of the image patch.\n        encoder_dim (int): Dimension of the encoder.\n        encoder_depth (int): Number of layers in the encoder.\n        encoder_heads (int): Number of attention heads in the encoder.\n        fusion_method (str): Fusion method to use. Defaults to \"mlp\", can be one of \"mlp\", \"concat\", \"add\", \"visual_expert\", \"matmul\", \"mobilevlm\", \"CrossAttention\".",
        "type": "code",
        "location": "/mm_mamba/model.py:1-25"
    },
    "39": {
        "file_id": 4,
        "content": "The code defines a class MMM for the MultiModalMamba model, which takes various arguments such as vocabulary size, dimension of dense vectors, and more. It consists of layers including MLP, VisualExpert, and MambaBlock from zeta.nn, along with Encoder and ViTransformerWrapper from zeta.structs. The fusion method can be set to different options like mlp, concat, add, etc.",
        "type": "comment"
    },
    "40": {
        "file_id": 4,
        "content": "        return_embeddings (bool): Whether to return the embeddings or not. Defaults to False.\n        expansion_ratio (int): Expansion ratio for the hidden dimension. Defaults to 4.\n        post_fuse_norm (bool): Whether to apply layer normalization after the fusion or not. Defaults to True.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Examples::\n    import torch\n    from mm_mamba.model import MMM\n    x = torch.randint(0, 10000, (1, 224))\n    img = torch.randn(1, 3, 224, 224)\n    model = MMM(\n        vocab_size=10000,\n        dim=512,\n        depth=6,\n        dropout=0.1,\n        heads=8,\n        d_state=512,\n        image_size=224,\n        patch_size=16,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n    )\n    out = model(x, img)\n    print(out.shape)\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        dim: int,\n        depth: int,\n        dropout: float,\n        heads: int,\n        d_state: int,\n        image_size: int,",
        "type": "code",
        "location": "/mm_mamba/model.py:26-67"
    },
    "41": {
        "file_id": 4,
        "content": "This code defines a class `MMM` for multi-modal modeling, which takes various input arguments such as vocab size, model dimensions, depth, dropout rate, etc. It also includes examples on how to use the class. The class likely implements a multi-modal transformer model with options for embedding return, expansion ratio, and post-fusion normalization.",
        "type": "comment"
    },
    "42": {
        "file_id": 4,
        "content": "        patch_size: int,\n        encoder_dim: int,\n        encoder_depth: int,\n        encoder_heads: int,\n        fusion_method: str = \"mlp\",\n        return_embeddings: bool = False,\n        expansion_ratio: int = 4,\n        post_fuse_norm: bool = True,\n        *args,\n        **kwargs,\n    ):\n        super(MMM, self).__init__()\n        self.vocab_size = vocab_size\n        self.dim = dim\n        self.depth = depth\n        self.dropout = dropout\n        self.heads = heads\n        self.d_state = d_state\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.encoder_dim = encoder_dim\n        self.encoder_depth = encoder_depth\n        self.encoder_heads = encoder_heads\n        self.fusion_method = fusion_method\n        self.return_embeddings = return_embeddings\n        self.expansion_ratio = expansion_ratio\n        self.post_fuse_norm = post_fuse_norm\n        # Transforms integer indices to dense vectors of fixed size\n        self.embedding = nn.Embedding(vocab_size, dim)\n        # MultiModalMambaBlock in a list",
        "type": "code",
        "location": "/mm_mamba/model.py:68-99"
    },
    "43": {
        "file_id": 4,
        "content": "This code defines a class \"MMM\" that extends some parent class. It takes various arguments to initialize its attributes such as patch_size, encoder_dim, encoder_depth, fusion_method, etc. It then initializes its own members including an embedding layer.",
        "type": "comment"
    },
    "44": {
        "file_id": 4,
        "content": "        self.layers = nn.ModuleList(\n            [\n                MambaBlock(\n                    dim,\n                    depth,\n                    d_state,\n                    expansion_ratio,\n                    *args,\n                    **kwargs,\n                )\n            ]\n        )\n        # Normalization layer\n        self.rmsnorm = RMSNorm(dim)\n        self.norm = nn.LayerNorm(dim)\n        # Linear layer\n        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n        # Tie weights\n        self.lm_head.weight = self.embedding.weight\n        # Projection for the img\n        self.img_proj = nn.Linear(dim, dim)\n        # Hidden dim\n        self.hidden_dim = dim * expansion_ratio\n        # Set up the ViT encoder\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # Setup the linear layer to project the image embeddings to the same dimension as the text embeddings",
        "type": "code",
        "location": "/mm_mamba/model.py:100-140"
    },
    "45": {
        "file_id": 4,
        "content": "This code defines a model with layers, normalization, linear layer, and projection for image. It uses the MambaBlock and ViTransformerWrapper for text and image encoding, respectively. The dimensions are defined based on input parameters.",
        "type": "comment"
    },
    "46": {
        "file_id": 4,
        "content": "        self.linear = nn.Linear(encoder_dim, dim)\n        # VisualExpert\n        self.visual_expert = VisualExpert(\n            dim, self.hidden_dim, dropout, heads\n        )\n        # MLP\n        self.mlp = MLP(\n            dim, dim, expansion_factor=4, depth=1, norm=True\n        )\n    def forward(self, text: Tensor, img: Tensor) -> Tensor:\n        \"\"\"\n        Forward pass of the MultiModalMamba model.\n        Args:\n            text (Tensor): Input text tensor.\n            img (Tensor): Input image tensor.\n        Returns:\n            Tensor: Output logits.\n        \"\"\"\n        x = self.embedding(text)\n        # print(f\"Text shape: {x.shape} inside the MMM\")\n        # Encode the image, Returns the same shape as text\n        encoded_img = self.encoder(img, return_embeddings=True)\n        # print(f\"Image shape: {encoded_img.shape} inside the MMM\")\n        # Project the image embeddings to the same dimension as the text embeddings\n        # We need to project the 2nd dim of the image embeddings to the same dimension as the text embeddings",
        "type": "code",
        "location": "/mm_mamba/model.py:141-171"
    },
    "47": {
        "file_id": 4,
        "content": "Code defines a MultiModalMamba model with embedding, encoder, visual expert, and MLP components. The forward pass takes text and image inputs, encodes the text, encodes the image, projects the image embeddings to match text dimension, and returns output logits.",
        "type": "comment"
    },
    "48": {
        "file_id": 4,
        "content": "        # if the fusion method is mlp, use the mlp to fuse the text and image embeddings\n        if self.fusion_method == \"mlp\":\n            fusion_layer = self.mlp(encoded_img)\n            fused = fusion_layer + x\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        # If fusion method is concat, concatenate the text and image embeddings\n        if self.fusion_method == \"concat\":\n            fused = torch.concat([x, encoded_img], dim=1)\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"add\":\n            fused = encoded_img + x\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"visual_expert\":\n            concat = torch.cat([x, encoded_img], dim=1)\n            fused = self.visual_expert(concat)\n            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        if self.fusion_method == \"matmul\":\n            fused = torch.matmul(encoded_img, x)",
        "type": "code",
        "location": "/mm_mamba/model.py:173-202"
    },
    "49": {
        "file_id": 4,
        "content": "This code implements a fusion method for combining text and image embeddings in a multimodal model. It supports several fusion techniques: MLP, concatenation, addition, visual expert, and matrix multiplication. If the post-fusion normalization is enabled, the fused representation is normalized.",
        "type": "comment"
    },
    "50": {
        "file_id": 4,
        "content": "            if self.post_fuse_norm:\n                fused = self.norm(fused)\n        # Need to implement this\n        if self.fusion_method == \"mobilevlm\":\n            pass\n        # Need to implement this\n        if self.fusion_method == \"CrossAttention\":\n            pass\n        x = fused\n        for layer in self.layers:\n            x = layer(x) + x\n        if self.return_embeddings:\n            return x\n        else:\n            x = self.norm(x)\n            logits = self.lm_head(x)\n            return logits",
        "type": "code",
        "location": "/mm_mamba/model.py:204-226"
    },
    "51": {
        "file_id": 4,
        "content": "This code segment initializes a fused representation and applies norm if post_fuse_norm is True. It needs to implement fusion methods \"mobilevlm\" and \"CrossAttention\". For each layer in layers, it passes the input through the layer and adds it to the previous result. If return_embeddings is True, it returns the final embedding; otherwise, it applies norm and feeds the output into lm_head for logits calculation.",
        "type": "comment"
    },
    "52": {
        "file_id": 5,
        "content": "/model_example.py",
        "type": "filepath"
    },
    "53": {
        "file_id": 5,
        "content": "This code imports the torch library, MMM model from mm_mamba module, generates random tensors x and img, creates a MMM object with specific parameters, passes x and img through the model and stores output in out, then prints out.",
        "type": "summary"
    },
    "54": {
        "file_id": 5,
        "content": "import torch  # Import the torch library\n# Import the MMM model from the mm_mamba module\nfrom mm_mamba import MMM\n# Generate a random tensor 'x' of size (1, 224) with random elements between 0 and 10000\nx = torch.randint(0, 10000, (1, 196))\n# Generate a random image tensor 'img' of size (1, 3, 224, 224)\nimg = torch.randn(1, 3, 224, 224)\n# Create a MMM model object with the following parameters:\nmodel = MMM(\n    vocab_size=10000,\n    dim=512,\n    depth=6,\n    dropout=0.1,\n    heads=8,\n    d_state=512,\n    image_size=224,\n    patch_size=16,\n    encoder_dim=512,\n    encoder_depth=6,\n    encoder_heads=8,\n    fusion_method=\"mlp\",\n    return_embeddings=False,\n    post_fuse_norm=True,\n)\n# Pass the tensor 'x' and 'img' through the model and store the output in 'out'\nout = model(x, img)\n# Print the shape of the output tensor 'out'\nprint(out.shape)",
        "type": "code",
        "location": "/model_example.py:1-34"
    },
    "55": {
        "file_id": 5,
        "content": "This code imports the torch library, MMM model from mm_mamba module, generates random tensors x and img, creates a MMM object with specific parameters, passes x and img through the model and stores output in out, then prints out.",
        "type": "comment"
    },
    "56": {
        "file_id": 6,
        "content": "/pyproject.toml",
        "type": "filepath"
    },
    "57": {
        "file_id": 6,
        "content": "This code configures Python project \"mmm-zeta\" using Poetry, specifying name, version, dependencies, authors, license, and more; requires Python 3.6+, torch v2.1.2, zetascale v1.4.0, einops. It also sets configurations for various development tools like Ruff, Autopep8, Black, Mypy-Protobuf with line lengths, target version, and preview mode.",
        "type": "summary"
    },
    "58": {
        "file_id": 6,
        "content": "[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n[tool.poetry]\nname = \"mmm-zeta\"\nversion = \"0.1.0\"\ndescription = \"MMM - Pytorch\"\nlicense = \"MIT\"\nauthors = [\"Kye Gomez <kye@apac.ai>\"]\nhomepage = \"https://github.com/kyegomez/MultiModalMamba\"\ndocumentation = \"https://github.com/kyegomez/MultiModalMamba\"  # Add this if you have documentation.\nreadme = \"README.md\"  # Assuming you have a README.md\nrepository = \"https://github.com/kyegomez/MultiModalMamba\"\nkeywords = [\"artificial intelligence\", \"deep learning\", \"optimizers\", \"Prompt Engineering\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3.9\"\n]\npackages = [\n    { include = \"mm_mamba\" },\n    { include = \"mm_mamba/**/*.py\" },\n]\n[tool.poetry.dependencies]\npython = \"^3.6\"\ntorch = \"2.1.2\"\nzetascale = \"1.4.0\"\neinops = \"*\"\n[tool.poetry.group.lint.dependencies]",
        "type": "code",
        "location": "/pyproject.toml:1-35"
    },
    "59": {
        "file_id": 6,
        "content": "This code snippet is a configuration for the Python project \"mmm-zeta\" using Poetry. It specifies the project's name, version, dependencies, authors, license, and more. The project relies on Python 3.6 or higher and requires torch (version 2.1.2), zetascale (version 1.4.0), and einops.",
        "type": "comment"
    },
    "60": {
        "file_id": 6,
        "content": "ruff = \"^0.1.6\"\ntypes-toml = \"^0.10.8.1\"\ntypes-redis = \"^4.3.21.6\"\ntypes-pytz = \"^2023.3.0.0\"\nblack = \"^23.1.0\"\ntypes-chardet = \"^5.0.4.6\"\nmypy-protobuf = \"^3.0.0\"\n[tool.autopep8]\nmax_line_length = 80\nignore = \"E501,W6\"  # or [\"E501\", \"W6\"]\nin-place = true\nrecursive = true\naggressive = 3\n[tool.ruff]\nline-length = 70\n[tool.black]\nline-length = 70\ntarget-version = ['py38']\npreview = true",
        "type": "code",
        "location": "/pyproject.toml:36-59"
    },
    "61": {
        "file_id": 6,
        "content": "This code is configuring various tools for Python development. It sets versions and settings for Ruff, Autopep8, Black, and Mypy-Protobuf. It also specifies line lengths for readability, target version for compatibility, and enables preview mode for certain tools.",
        "type": "comment"
    },
    "62": {
        "file_id": 7,
        "content": "/requirements.txt",
        "type": "filepath"
    },
    "63": {
        "file_id": 7,
        "content": "This code specifies the required package versions for torch, zetascale, and swarms in a Python project. The versions are 2.1.2 for torch, 1.4.0 for zetascale, and 3.4.2 for swarms.",
        "type": "summary"
    },
    "64": {
        "file_id": 7,
        "content": "torch==2.1.2\nzetascale==1.4.0\nswarms==3.4.2",
        "type": "code",
        "location": "/requirements.txt:1-3"
    },
    "65": {
        "file_id": 7,
        "content": "This code specifies the required package versions for torch, zetascale, and swarms in a Python project. The versions are 2.1.2 for torch, 1.4.0 for zetascale, and 3.4.2 for swarms.",
        "type": "comment"
    },
    "66": {
        "file_id": 8,
        "content": "/scripts/auto_tests_docs/auto_docs.py",
        "type": "filepath"
    },
    "67": {
        "file_id": 8,
        "content": "The code utilizes an OpenAI model to process class documentation and outputs it in a Markdown format, creates necessary directories, processes classes concurrently with threading, and prints generated documentation. It ensures proper execution when run directly by defining a main function and checking for script importation.",
        "type": "summary"
    },
    "68": {
        "file_id": 8,
        "content": "###### VERISON2\nimport inspect\nimport os\nimport threading\nfrom dotenv import load_dotenv\nfrom scripts.auto_tests_docs.docs import DOCUMENTATION_WRITER_SOP\nfrom swarms import OpenAIChat\n##########\nfrom zeta.nn.modules.quantized_layernorm import QuantizedLN\nfrom zeta.nn.modules.slerp_model_merger import SLERPModelMerger\nfrom zeta.nn.modules.avg_model_merger import AverageModelMerger\n####################\nload_dotenv()\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = OpenAIChat(\n    model_name=\"gpt-4\",\n    openai_api_key=api_key,\n    max_tokens=3000,\n)\ndef process_documentation(cls):\n    \"\"\"\n    Process the documentation for a given class using OpenAI model and save it in a Markdown file.\n    \"\"\"\n    doc = inspect.getdoc(cls)\n    source = inspect.getsource(cls)\n    input_content = (\n        \"Class Name:\"\n        f\" {cls.__name__}\\n\\nDocumentation:\\n{doc}\\n\\nSource\"\n        f\" Code:\\n{source}\"\n    )\n    # Process with OpenAI model (assuming the model's __call__ method takes this input and returns processed content)\n    processed_content = model(",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_docs.py:1-41"
    },
    "69": {
        "file_id": 8,
        "content": "This code processes the documentation for a given class using an OpenAI model, then saves it in a Markdown file. It first imports necessary modules and loads environment variables. The `process_documentation` function takes a class as input, gets its documentation and source code, creates an input string combining these, passes this to the OpenAI model for processing, and saves the processed content in a Markdown file.",
        "type": "comment"
    },
    "70": {
        "file_id": 8,
        "content": "        DOCUMENTATION_WRITER_SOP(input_content, \"zeta.nn.modules\")\n    )\n    # doc_content = f\"# {cls.__name__}\\n\\n{processed_content}\\n\"\n    doc_content = f\"{processed_content}\\n\"\n    # Create the directory if it doesn't exist\n    dir_path = \"docs/zeta/nn/modules\"\n    os.makedirs(dir_path, exist_ok=True)\n    # Write the processed documentation to a Markdown file\n    file_path = os.path.join(dir_path, f\"{cls.__name__.lower()}.md\")\n    with open(file_path, \"w\") as file:\n        file.write(doc_content)\n    print(f\"Documentation generated for {cls.__name__}.\")\ndef main():\n    classes = [\n        QuantizedLN,\n        SLERPModelMerger,\n        AverageModelMerger,\n    ]\n    threads = []\n    for cls in classes:\n        thread = threading.Thread(\n            target=process_documentation, args=(cls,)\n        )\n        threads.append(thread)\n        thread.start()\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    print(\n        \"Documentation generated in 'docs/zeta/nn/modules' directory.\"",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_docs.py:42-80"
    },
    "71": {
        "file_id": 8,
        "content": "This code generates documentation for specified classes. It creates a directory if it doesn't exist, writes processed documentation to a Markdown file within the directory, and prints the generated documentation for each class. The code uses threading to process multiple classes concurrently.",
        "type": "comment"
    },
    "72": {
        "file_id": 8,
        "content": "    )\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_docs.py:81-85"
    },
    "73": {
        "file_id": 8,
        "content": "The code is defining a main function and checking if the script is being run directly (not imported), then calling the main function if so.",
        "type": "comment"
    },
    "74": {
        "file_id": 9,
        "content": "/scripts/auto_tests_docs/auto_docs_functions.py",
        "type": "filepath"
    },
    "75": {
        "file_id": 9,
        "content": "The given code utilizes an OpenAI model to generate Markdown documentation for functions in the zeta.ops module, saving it concurrently in a designated directory using threading and exception handling.",
        "type": "summary"
    },
    "76": {
        "file_id": 9,
        "content": "import inspect\nimport os\nimport sys\nimport threading\nfrom dotenv import load_dotenv\nfrom scripts.auto_tests_docs.docs import DOCUMENTATION_WRITER_SOP\nfrom swarms import OpenAIChat\nfrom zeta.ops import *\nload_dotenv()\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = OpenAIChat(\n    model_name=\"gpt-4-1106-preview\",\n    openai_api_key=api_key,\n    max_tokens=2000,\n)\ndef process_documentation(item):\n    \"\"\"\n    Process the documentation for a given function using OpenAI model and save it in a Markdown file.\n    \"\"\"\n    try:\n        doc = inspect.getdoc(item)\n        source = inspect.getsource(item)\n        input_content = (\n            \"Name:\"\n            f\" {item.__name__}\\n\\nDocumentation:\\n{doc}\\n\\nSource\"\n            f\" Code:\\n{source}\"\n        )\n        # Process with OpenAI model\n        processed_content = model(\n            DOCUMENTATION_WRITER_SOP(input_content, \"zeta.ops\")\n        )\n        doc_content = f\"# {item.__name__}\\n\\n{processed_content}\\n\"\n        # Create the directory if it doesn't exist\n        dir_path = \"docs/zeta/ops\"",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_docs_functions.py:1-44"
    },
    "77": {
        "file_id": 9,
        "content": "This code processes the documentation for a given function using an OpenAI model and saves it in a Markdown file. It first retrieves the documentation and source code of the function, then inputs this information into the OpenAI model for processing. The processed content is then formatted as a Markdown document and saved to the \"docs/zeta/ops\" directory if it doesn't already exist.",
        "type": "comment"
    },
    "78": {
        "file_id": 9,
        "content": "        os.makedirs(dir_path, exist_ok=True)\n        # Write the processed documentation to a Markdown file\n        file_path = os.path.join(\n            dir_path, f\"{item.__name__.lower()}.md\"\n        )\n        with open(file_path, \"w\") as file:\n            file.write(doc_content)\n        print(f\"Succesfully processed {item.__name__}.\")\n    except Exception as e:\n        print(f\"Error processing {item.__name__}: {e}\")\ndef main():\n    # Gathering all functions from the zeta.ops module\n    functions = [\n        obj\n        for name, obj in inspect.getmembers(sys.modules[\"zeta.ops\"])\n        if inspect.isfunction(obj)\n    ]\n    threads = []\n    for func in functions:\n        thread = threading.Thread(\n            target=process_documentation, args=(func,)\n        )\n        threads.append(thread)\n        thread.start()\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    print(\"Documentation generated in 'docs/zeta/ops' directory.\")\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_docs_functions.py:45-83"
    },
    "79": {
        "file_id": 9,
        "content": "This code generates documentation for functions in the zeta.ops module, creating Markdown files with processed documentation in a designated directory. It utilizes threading to process each function concurrently, and handles exceptions during processing. The generated documentation is stored in the 'docs/zeta/ops' directory.",
        "type": "comment"
    },
    "80": {
        "file_id": 10,
        "content": "/scripts/auto_tests_docs/auto_tests.py",
        "type": "filepath"
    },
    "81": {
        "file_id": 10,
        "content": "This code utilizes OpenAI GPT-4 to automate test and documentation generation for classes or functions, efficiently extracting Markdown code in parallel threads.",
        "type": "summary"
    },
    "82": {
        "file_id": 10,
        "content": "import inspect\nimport os\nimport re\nimport threading\nfrom swarms import OpenAIChat\nfrom scripts.auto_tests_docs.docs import TEST_WRITER_SOP_PROMPT\n# Import all classes from zeta.structs\n# Tests will be automatically generated in the tests folder using parallized gpt4 with each of the file logic handled autonomously thus\n# leading to a much faster testing process where you just import your classes or functions and tests are automatically generated\n# Automating tests and documentation frees up atleast 75% of your time to focus on the actual logic of your code\nfrom zeta.nn.modules.triple_skip import TripleSkipBlock\nfrom zeta.nn.modules.dynamic_routing_block import DynamicRoutingBlock\nfrom zeta.nn.modules.gated_residual_block import GatedResidualBlock\nfrom zeta.nn.modules.stochastic_depth import StochasticSkipBlocK\n####################\nfrom dotenv import load_dotenv\nload_dotenv()\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = OpenAIChat(\n    model_name=\"gpt-4\",\n    openai_api_key=api_key,\n    max_tokens=500,\n)\ndef extract_code_from_markdown(markdown_content: str):",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests.py:1-35"
    },
    "83": {
        "file_id": 10,
        "content": "This code imports necessary modules and classes, sets up an OpenAI API instance for communication with GPT-4, and defines a function to extract code from Markdown content. The purpose is to automate the generation of tests and documentation for classes or functions by importing them, leading to faster testing and more time to focus on code logic.",
        "type": "comment"
    },
    "84": {
        "file_id": 10,
        "content": "    \"\"\"\n    Extracts code blocks from a Markdown string and returns them as a single string.\n    Args:\n    - markdown_content (str): The Markdown content as a string.\n    Returns:\n    - str: A single string containing all the code blocks separated by newlines.\n    \"\"\"\n    # Regular expression for fenced code blocks\n    pattern = r\"```(?:\\w+\\n)?(.*?)```\"\n    matches = re.findall(pattern, markdown_content, re.DOTALL)\n    # Concatenate all code blocks separated by newlines\n    return \"\\n\".join(code.strip() for code in matches)\ndef create_test(cls):\n    \"\"\"\n    Process the documentation for a given class using OpenAI model and save it in a Python file.\n    \"\"\"\n    doc = inspect.getdoc(cls)\n    source = inspect.getsource(cls)\n    input_content = (\n        \"Class Name:\"\n        f\" {cls.__name__}\\n\\nDocumentation:\\n{doc}\\n\\nSource\"\n        f\" Code:\\n{source}\"\n    )\n    # Process with OpenAI model (assuming the model's __call__ method takes this input and returns processed content)\n    processed_content = model(\n        TEST_WRITER_SOP_PROMPT(",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests.py:36-67"
    },
    "85": {
        "file_id": 10,
        "content": "The code snippet is a function that extracts and concatenates code blocks from a Markdown string, and another function that processes the documentation of a class using an OpenAI model and saves it in a Python file.",
        "type": "comment"
    },
    "86": {
        "file_id": 10,
        "content": "            input_content, \"zeta\", \"zeta.nn.modules\"\n        )\n    )\n    processed_content = extract_code_from_markdown(processed_content)\n    doc_content = f\"{processed_content}\"\n    # Create the directory if it doesn't exist\n    dir_path = \"tests/nn/modules\"\n    os.makedirs(dir_path, exist_ok=True)\n    # Write the processed documentation to a Python file\n    file_path = os.path.join(dir_path, f\"{cls.__name__.lower()}.py\")\n    with open(file_path, \"w\") as file:\n        file.write(doc_content)\n    print(f\"Test generated for {cls.__name__}.\")\ndef main():\n    classes = [\n        TripleSkipBlock,\n        DynamicRoutingBlock,\n        GatedResidualBlock,\n        StochasticSkipBlocK,\n    ]\n    threads = []\n    for cls in classes:\n        thread = threading.Thread(target=create_test, args=(cls,))\n        threads.append(thread)\n        thread.start()\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    print(\"Tests generated in 'tests/nn/modules' directory.\")\nif __name__ == \"__main__\":",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests.py:68-108"
    },
    "87": {
        "file_id": 10,
        "content": "The code creates test files for various classes (TripleSkipBlock, DynamicRoutingBlock, GatedResidualBlock, StochasticSkipBlocK) in the \"tests/nn/modules\" directory. It generates a separate Python file for each class by extracting the code from Markdown and creating the necessary directories if they don't exist already. The main function runs the test generation in multiple threads to speed up the process.",
        "type": "comment"
    },
    "88": {
        "file_id": 10,
        "content": "    main()",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests.py:109-109"
    },
    "89": {
        "file_id": 10,
        "content": "The code is calling the main function.",
        "type": "comment"
    },
    "90": {
        "file_id": 11,
        "content": "/scripts/auto_tests_docs/auto_tests_functions.py",
        "type": "filepath"
    },
    "91": {
        "file_id": 11,
        "content": "This code uses OpenAI's gpt-4 model to process documentation and source code, saving the processed information in Markdown files. It automates testing or documentation generation for Python modules.",
        "type": "summary"
    },
    "92": {
        "file_id": 11,
        "content": "import inspect\nimport os\nimport sys\nimport threading\nfrom dotenv import load_dotenv\nfrom scripts.auto_tests_docs.docs import TEST_WRITER_SOP_PROMPT\nfrom swarms import OpenAIChat\nfrom swarms.utils.parse_code import (\n    extract_code_from_markdown,\n)\nfrom zeta.utils import *\nload_dotenv()\napi_key = os.getenv(\"OPENAI_API_KEY\")\nmodel = OpenAIChat(\n    model_name=\"gpt-4\",\n    openai_api_key=api_key,\n    max_tokens=4000,\n)\ndef process_documentation(item):\n    \"\"\"\n    Process the documentation for a given function using OpenAI model and save it in a Markdown file.\n    \"\"\"\n    doc = inspect.getdoc(item)\n    source = inspect.getsource(item)\n    input_content = (\n        f\"Name: {item.__name__}\\n\\nDocumentation:\\n{doc}\\n\\nSource\"\n        f\" Code:\\n{source}\"\n    )\n    # print(input_content)\n    # Process with OpenAI model\n    processed_content = model(\n        TEST_WRITER_SOP_PROMPT(\n            input_content, \"zeta.utils\", \"zeta.utils\"\n        )\n    )\n    processed_content = extract_code_from_markdown(processed_content)\n    doc_content = f\"{processed_content}\"",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests_functions.py:1-46"
    },
    "93": {
        "file_id": 11,
        "content": "This code imports necessary libraries, loads an OpenAI API key, initializes the OpenAI model (gpt-4), and defines a function called \"process_documentation\". The function takes an input item, extracts its documentation and source code, processes this information using the OpenAI model to generate relevant content, and saves it in a Markdown file. The extracted code is also cleaned up for easier reading. This appears to be part of some automated testing or documentation generation process.",
        "type": "comment"
    },
    "94": {
        "file_id": 11,
        "content": "    # Create the directory if it doesn't exist\n    dir_path = \"tests/utils\"\n    os.makedirs(dir_path, exist_ok=True)\n    # Write the processed documentation to a Markdown file\n    file_path = os.path.join(dir_path, f\"{item.__name__.lower()}.py\")\n    with open(file_path, \"w\") as file:\n        file.write(doc_content)\n    print(f\"Test generated for {item.__name__}.\")\ndef main():\n    # Gathering all functions from the zeta.utils module\n    functions = [\n        obj\n        for name, obj in inspect.getmembers(sys.modules[\"zeta.utils\"])\n        if inspect.isfunction(obj)\n    ]\n    threads = []\n    for func in functions:\n        thread = threading.Thread(\n            target=process_documentation, args=(func,)\n        )\n        threads.append(thread)\n        thread.start()\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    print(\"Tests generated in 'tests/utils' directory.\")\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/scripts/auto_tests_docs/auto_tests_functions.py:48-84"
    },
    "95": {
        "file_id": 11,
        "content": "The code creates a directory and writes processed documentation to Markdown files for each function in the zeta.utils module using multiple threads.",
        "type": "comment"
    },
    "96": {
        "file_id": 12,
        "content": "/scripts/auto_tests_docs/docs.py",
        "type": "filepath"
    },
    "97": {
        "file_id": 12,
        "content": "This code produces professional documentation in markdown format and offers a function for multi-head attention computation, discussing robust Python test best practices and pytest optimization.",
        "type": "summary"
    },
    "98": {
        "file_id": 12,
        "content": "def DOCUMENTATION_WRITER_SOP(\n    task: str,\n    module: str,\n):\n    documentation = f\"\"\"Create multi-page long and explicit professional pytorch-like documentation for the {module} code below follow the outline for the {module} library,\n    provide many examples and teach the user about the code, provide examples for every function, make the documentation 10,000 words,\n    provide many usage examples and note this is markdown docs, create the documentation for the code to document,\n    put the arguments and methods in a table in markdown to make it visually seamless\n    Now make the professional documentation for this code, provide the architecture and how the class works and why it works that way,\n    it's purpose, provide args, their types, 3 ways of usage examples, in examples show all the code like imports main example etc\n    BE VERY EXPLICIT AND THOROUGH, MAKE IT DEEP AND USEFUL\n    ########\n    Step 1: Understand the purpose and functionality of the module or framework\n    Read and analyze ",
        "type": "code",
        "location": "/scripts/auto_tests_docs/docs.py:1-18"
    },
    "99": {
        "file_id": 12,
        "content": "This function generates a multi-page, long, and explicit professional documentation for a given module or framework using markdown format. It requires the task (module) name and the module name as inputs. The generated documentation will include clear explanations, usage examples, tables for arguments and methods, and a thorough understanding of the code's architecture and purpose.",
        "type": "comment"
    }
}